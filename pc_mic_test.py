import pyaudio
import numpy as np
import tensorflow as tf
from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
import collections
import time

# --- 1. íŒŒë¼ë¯¸í„° ì„¤ì • ---
SAMPLE_RATE = 16000
CHUNK_DURATION_MS = 250  # 0.25ì´ˆë§ˆë‹¤ ë§ˆì´í¬ì—ì„œ ê°€ì ¸ì˜´
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)  # 4000 samples
WINDOW_SIZE_MS = 40.0
WINDOW_STRIDE_MS = 20.0
DCT_COEFFICIENT_COUNT = 10
CLIP_DURATION_SAMPLES = SAMPLE_RATE  # 1ì´ˆ ë¶„ëŸ‰ (16000)

LABELS = ['_silence_', '_unknown_', 'yes', 'no']

# --- 2. TFLite ëª¨ë¸ ë¡œë”© ---
interpreter = tf.lite.Interpreter(model_path="./work/ds_cnn.tflite")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# --- 3. ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ(MFCC)ì„ ìœ„í•œ TF ê·¸ë˜í”„ êµ¬ì¶• ---
tf.reset_default_graph()
sess = tf.Session()

wav_placeholder = tf.placeholder(tf.float32, [CLIP_DURATION_SAMPLES, 1])
spectrogram = contrib_audio.audio_spectrogram(
    wav_placeholder,
    window_size=int(SAMPLE_RATE * WINDOW_SIZE_MS / 1000),
    stride=int(SAMPLE_RATE * WINDOW_STRIDE_MS / 1000),
    magnitude_squared=True)
mfcc_op = contrib_audio.mfcc(
    spectrogram,
    SAMPLE_RATE,
    dct_coefficient_count=DCT_COEFFICIENT_COUNT)
# output shape of mfcc: [1, 49, 10] (if 1 sec window). => Flatten to [1, 490]
mfcc_flatten = tf.reshape(mfcc_op, [1, -1])

# --- 4. ì•ˆì •ì„± ë¡œì§ ì„¸íŒ… ---
window_history = collections.deque(maxlen=2)  # ìµœê·¼ 4ë²ˆ(1ì´ˆ)ì˜ ê²°ê³¼ë¥¼ í‰ê·  ë‚´ì–´ ìŠ¤ë¬´ë”©
suppression_counter = 0                       # ê°ì§€ í›„ ì¼ì • ì‹œê°„ ë™ì•ˆ ì¤‘ë³µ ê°ì§€ ë¬´ì‹œ
SUPPRESSION_PULL_DOWN = 6                     # ê°ì§€ í›„ 6í‹±(1.5ì´ˆ) ë™ì•ˆì€ ë¬´ì‹œ

# --- 5. ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¬ë° ì„¸íŒ… ---
# 1ì´ˆ ë¶„ëŸ‰(16000 íŒ¨ë”©)ì˜ ë²„í¼ í ìƒì„±
audio_buffer = np.zeros(CLIP_DURATION_SAMPLES, dtype=np.float32)

p = pyaudio.PyAudio()
# paInt16ìœ¼ë¡œ ë°›ê³  ìˆ˜ë™ ì •ê·œí™”í•˜ëŠ” ê²ƒì´ ì•ˆì „
stream = p.open(format=pyaudio.paInt16,
                channels=1,
                rate=SAMPLE_RATE,
                input=True,
                frames_per_buffer=CHUNK_SIZE)

print("\n==== ğŸ¤ ì‹¤ì‹œê°„ ë§ˆì´í¬ KWS ì‹œì‘ ====")
print("('yes' ë˜ëŠ” 'no'ë¥¼ ë§í•´ë³´ì„¸ìš”!)")
print("===================================")

try:
    while True:
        # ë§ˆì´í¬ë¡œë¶€í„° ì²­í¬(0.25ì´ˆ ë¶„ëŸ‰) ì½ê¸°
        data = stream.read(CHUNK_SIZE, exception_on_overflow=False)
        # Int16 -> [-1.0, 1.0] ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ì™„ë²½ ë™ì¼)
        audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
        
        # ìŒì„± ë³¼ë¥¨ ì²´í¬ (ë§ˆì´í¬ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸ìš©)
        volume = np.max(np.abs(audio_chunk))
        
        # ì´ì „ ìŒì„±ì„ ë’¤ë¡œ ë°€ê³ , ìƒˆë¡œìš´ ìŒì„±ì„ ì•ì— ì¶”ê°€ (Sliding Window ë°©ì‹)
        audio_buffer = np.roll(audio_buffer, -CHUNK_SIZE)
        audio_buffer[-CHUNK_SIZE:] = audio_chunk
        
        # --- (A) 1ì´ˆ ë¶„ëŸ‰ ì˜¤ë””ì˜¤ -> MFCC ë³€í™˜ ---
        feed_dict = {wav_placeholder: audio_buffer.reshape(-1, 1)}
        fingerprint = sess.run(mfcc_flatten, feed_dict=feed_dict)
        
        # --- (B) MFCC -> TFLite ì¶”ë¡  ---
        interpreter.set_tensor(input_details[0]['index'], fingerprint)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])[0]
        
        # íˆìŠ¤í† ë¦¬ì— í˜„ì¬ ì˜ˆì¸¡ ì €ì¥
        window_history.append(output_data)
        
        # --- (C) ì˜ˆì¸¡ ìŠ¤ë¬´ë”© (Smoothing) ë° í¬ì°© ë¡œì§ ---
        if len(window_history) == window_history.maxlen:
            # ìµœê·¼ ê¸°ë¡ì˜ í‰ê· ì„ êµ¬í•´ íŠ€ëŠ” ê°’ì„ ë°©ì§€
            smoothed_output = np.mean(window_history, axis=0)
            top_index = np.argmax(smoothed_output)
            top_score = smoothed_output[top_index]
            prediction = LABELS[top_index]
            
            # ì–µì œ(Suppression) ì§„í–‰ ì¤‘ì´ë©´ ì¹´ìš´í„° ê°ì†Œ
            if suppression_counter > 0:
                suppression_counter -= 1
                msg = f"  (ê°ì§€ ëŒ€ê¸° ì¤‘... ë³¼ë¥¨: {volume:.2f})           "
                print(msg, end='\r', flush=True)
                continue
            
            # ë³¼ë¥¨ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ë°°ê²½ ì¡ìŒìœ¼ë¡œ ê°„ì£¼
            if volume < 0.05:
                prediction = '_silence_'
            
            # í¬ì°© (Smoothingëœ ì‹ ë¢°ë„ê°€ 80% ì´ìƒì´ê³  íƒ€ê¹ƒì¼ ë•Œ)
            if top_score > 0.8 and prediction in ['yes', 'no']:
                print(f"ğŸ”¥ í¬ì°©ë¨: '{prediction}' (ì‹ ë¢°ë„: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})")
                suppression_counter = SUPPRESSION_PULL_DOWN  # ì¤‘ë³µ í¬ì°© ë°©ì§€
            else:
                msg = f"  ({prediction}: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})           "
                print(msg, end='\r', flush=True)

except KeyboardInterrupt:
    print("\n==== ğŸ›‘ ë§ˆì´í¬ KWS ì¢…ë£Œ ====")
finally:
    stream.stop_stream()
    stream.close()
    p.terminate()
    sess.close()
