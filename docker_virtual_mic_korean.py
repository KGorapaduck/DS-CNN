import wave
import numpy as np
import tensorflow as tf
from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
import collections
import time
import os

# --- 1. íŒŒë¼ë¯¸í„° ì„¤ì • ---
SAMPLE_RATE = 16000
CHUNK_DURATION_MS = 250  # 0.25ì´ˆë§ˆë‹¤ ëª¨ë¸ ì¶”ë¡  (Overlap)
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)  # 4000 samples
WINDOW_SIZE_MS = 40.0
WINDOW_STRIDE_MS = 20.0
DCT_COEFFICIENT_COUNT = 10
CLIP_DURATION_SAMPLES = SAMPLE_RATE  # ëª¨ë¸ì˜ ì…ë ¥ ì‚¬ì´ì¦ˆëŠ” ë¬´ì¡°ê±´ 1ì´ˆ (16000)

LABELS = ['_silence_', '_unknown_', 'quiz', 'understand']
WAV_FILE_PATH = "korean_virtual_mic_test.wav"  # ì‚¬ìš©ìê°€ ë¼ì¦ˆë² ë¦¬íŒŒì´ë¡œ ì˜®ê¸¸ íŒŒì¼ ì´ë¦„
PB_MODEL_PATH = "work/ds_cnn_korean_frozen.pb"

if not os.path.exists(PB_MODEL_PATH):
    PB_MODEL_PATH = "ds_cnn_korean_frozen.pb" # ë¼ì¦ˆë² ë¦¬íŒŒì´ ê°™ì€ í´ë”ì— ìˆì„ ê²½ìš° ëŒ€ë¹„

if not os.path.exists(WAV_FILE_PATH):
    print(f"âŒ ì˜¤ë¥˜: '{WAV_FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    print("ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ê²½ë¡œ(í˜„ì¬ í´ë”)ì— í•´ë‹¹ wav íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    print("PCì— ìˆëŠ” ë…¹ìŒ íŒŒì¼ì„ ë³µì‚¬í•´ì„œ ê°€ì ¸ì˜¤ì„¸ìš”!")
    exit(1)

if not os.path.exists(PB_MODEL_PATH):
    print(f"âŒ ì˜¤ë¥˜: '{PB_MODEL_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    print("ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ê²½ë¡œ(í˜„ì¬ í´ë”)ì— í•´ë‹¹ pb íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    exit(1)

# --- 2. TF ì„¸ì…˜ ë° ê·¸ë˜í”„ êµ¬ì¶• ---
tf.reset_default_graph()
sess = tf.compat.v1.InteractiveSession()

wav_placeholder = tf.placeholder(tf.float32, [CLIP_DURATION_SAMPLES, 1])

# MFCC ì¶”ì¶œ ê·¸ë˜í”„ (contrib_audio)
spectrogram = contrib_audio.audio_spectrogram(
    wav_placeholder,
    window_size=int(SAMPLE_RATE * WINDOW_SIZE_MS / 1000),
    stride=int(SAMPLE_RATE * WINDOW_STRIDE_MS / 1000),
    magnitude_squared=True)
mfcc_op = contrib_audio.mfcc(
    spectrogram,
    SAMPLE_RATE,
    dct_coefficient_count=DCT_COEFFICIENT_COUNT)
mfcc_flatten = tf.reshape(mfcc_op, [1, -1])

# --- 3. Frozen Graph(.pb) ë¡œë“œ ---
graph_def = tf.compat.v1.GraphDef()
with tf.io.gfile.GFile(PB_MODEL_PATH, 'rb') as f:
    graph_def.ParseFromString(f.read())
    
tf.import_graph_def(graph_def, name='frozen_model')

model_graph = tf.compat.v1.get_default_graph()
fingerprint_input = model_graph.get_tensor_by_name("frozen_model/Reshape:0")
probabilities_op = model_graph.get_tensor_by_name("frozen_model/labels_softmax:0")

# --- 4. ì•ˆì •ì„± ë¡œì§ ì„¸íŒ… (PC ë§ˆì´í¬ ìµœì í™”ì™€ ë™ì¼) ---
window_history = collections.deque(maxlen=2)
suppression_counter = 0                       
SUPPRESSION_PULL_DOWN = 6                     

# 1ì´ˆ ë¶„ëŸ‰(16000)ì˜ ë¹ˆ ë²„í¼(ë°°ê²½ìŒ)
audio_buffer = np.zeros(CLIP_DURATION_SAMPLES, dtype=np.float32)

print(f"\n==== ğŸ§ ê°€ìƒ ë§ˆì´í¬(WAV) ì½ê¸° ì‹œì‘ (Docker + .pb) ====")
print(f"ëª¨ë¸ íŒŒì¼: {PB_MODEL_PATH}")
print(f"ì¬ìƒ íŒŒì¼: {WAV_FILE_PATH}")
print("========================================================\n")

wf = wave.open(WAV_FILE_PATH, 'rb')
elapsed_time_ms = 0

def format_time(ms):
    """ë°€ë¦¬ì´ˆë¥¼ ë¶„:ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    ms = int(ms)
    minutes = ms // 60000
    seconds = (ms % 60000) // 1000
    milliseconds = ms % 1000
    return f"{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

try:
    while True:
        # 1. ê°€ìƒ ë§ˆì´í¬(WAV íŒŒì¼)ì—ì„œ 0.25ì´ˆ ë¶„ëŸ‰(4000ê°œ ìƒ˜í”Œ)ì”© ì˜ë¼ì„œ ì½ê¸°
        data = wf.readframes(CHUNK_SIZE)
        
        # íŒŒì¼ì´ ëë‚¬ìœ¼ë©´ ì¢…ë£Œ
        if len(data) == 0:
            print("\n==== ğŸ WAV íŒŒì¼ ì¬ìƒ ì™„ë£Œ ====")
            break
            
        # í”„ë ˆì„ ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš°(íŒŒì¼ì˜ ë§¨ ëë¶€ë¶„) ì œë¡œ íŒ¨ë”©ìœ¼ë¡œ í¬ê¸° ë§ì¶”ê¸°
        audio_chunk_int16 = np.frombuffer(data, dtype=np.int16)
        if len(audio_chunk_int16) < CHUNK_SIZE:
            padded_chunk = np.zeros(CHUNK_SIZE, dtype=np.int16)
            padded_chunk[:len(audio_chunk_int16)] = audio_chunk_int16
            audio_chunk_int16 = padded_chunk
            
        # Int16 -> Float32 [-1.0, 1.0] ì •ê·œí™”
        audio_chunk = audio_chunk_int16.astype(np.float32) / 32768.0
        volume = np.max(np.abs(audio_chunk))
        
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: ë²„í¼ ì•ˆì˜ ë‚´ìš©ë¬¼ì„ ì™¼ìª½ìœ¼ë¡œ 0.25ì´ˆ ë°€ê³  ìƒˆ 0.25ì´ˆ ì±„ìš°ê¸°
        audio_buffer = np.roll(audio_buffer, -CHUNK_SIZE)
        audio_buffer[-CHUNK_SIZE:] = audio_chunk
        
        # --- (A) 1ì´ˆ ë¶„ëŸ‰ ì˜¤ë””ì˜¤ -> MFCC ë³€í™˜ ---
        mfcc_feat = sess.run(mfcc_flatten, feed_dict={wav_placeholder: audio_buffer.reshape(-1, 1)})
        
        # --- (B) MFCC -> DS-CNN(.pb) ì¶”ë¡  ---
        probs = sess.run(probabilities_op, feed_dict={fingerprint_input: mfcc_feat})[0]
        
        window_history.append(probs)
        
        # --- (C) ì˜ˆì¸¡ ìŠ¤ë¬´ë”© ë° íŒì • ---
        if len(window_history) == window_history.maxlen:
            smoothed_output = np.mean(window_history, axis=0)
            top_index = np.argmax(smoothed_output)
            top_score = smoothed_output[top_index]
            prediction = LABELS[top_index]
            
            if suppression_counter > 0:
                suppression_counter -= 1
                msg = f"  (ê°ì§€ ë³´ë¥˜ ì¤‘... ë³¼ë¥¨: {volume:.2f})"
                print(msg.ljust(50), end='\r', flush=True)
                continue
            
            if volume < 0.02:
                prediction = '_silence_'
            
            # Threshold ì°¨ë“± ì ìš© (í€´ì¦ˆ: 0.6, ì´í•´: 0.3)
            if (prediction == 'quiz' and top_score >= 0.6) or (prediction == 'understand' and top_score >= 0.3):
                timestamp = format_time(elapsed_time_ms)
                print(f"ğŸ”¥ [{timestamp}] í¬ì°©ë¨: '{prediction}' (ì‹ ë¢°ë„: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})")
                
                # ğŸ’¡ ì†Œì¼“ í˜¹ì€ HTTP API í†µì‹ ì„ í•˜ë ¤ë©´ ì—¬ê¸°ì— í†µì‹  ì½”ë“œë¥¼ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤!
                # if prediction == 'quiz':
                #    socket.send("TRIGGER_QUIZ")
                
                suppression_counter = SUPPRESSION_PULL_DOWN
            else:
                msg = f"  ({prediction}: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})"
                print(msg.ljust(50), end='\r', flush=True)
                    
        # ê°€ìƒ ë§ˆì´í¬ì²˜ëŸ¼ ë³´ì´ë„ë¡ ì˜ë„ì ì¸ ì‹œê°„ ì§€ì—° (1ë°°ì† ì¬ìƒ)
        time.sleep(0.25)
        elapsed_time_ms += CHUNK_DURATION_MS

except KeyboardInterrupt:
    print("\n==== ğŸ›‘ ê°•ì œ ì¢…ë£Œ ====")
finally:
    wf.close()
    sess.close()
