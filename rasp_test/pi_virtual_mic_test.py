import wave
import numpy as np
import time
from python_speech_features import mfcc
# ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œëŠ” tensorflow ì „ì²´ ëŒ€ì‹  ê°€ë²¼ìš´ tflite_runtimeì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    print("tflite_runtimeì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TFLite ëŒ€ì‹  ì¼ë°˜ tensorflowë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
    import tensorflow as tf
    tflite = tf.lite

# --- 1. íŒŒë¼ë¯¸í„° ì„¤ì • ---
SAMPLE_RATE = 16000
CHUNK_DURATION_MS = 250  # 0.25ì´ˆ ë‹¨ìœ„
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)  # 4000 samples
WINDOW_SIZE_MS = 40.0
WINDOW_STRIDE_MS = 20.0
DCT_COEFFICIENT_COUNT = 10
CLIP_DURATION_SAMPLES = SAMPLE_RATE  # 1ì´ˆ ë¶„ëŸ‰ (16000)

LABELS = ['_silence_', '_unknown_', 'yes', 'no']

# ë¼ì¦ˆë² ë¦¬íŒŒì´ì— ë„£ì€ ì˜¤ë””ì˜¤ ë° ëª¨ë¸ ê²½ë¡œ (ê²½ë¡œëŠ” ë¼ì¦ˆë² ë¦¬íŒŒì´ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
VIRTUAL_MIC_WAV = "virtual_mic_test.wav"
TFLITE_MODEL = "ds_cnn.tflite"

# --- 2. TFLite ëª¨ë¸ ë¡œë”© ---
interpreter = tflite.Interpreter(model_path=TFLITE_MODEL)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# --- 3. ì•ˆì •ì„± ë¡œì§ ì„¸íŒ… ---
# ìµœê·¼ 4ë²ˆ(1ì´ˆ)ì˜ ê²°ê³¼ë¥¼ í‰ê·  ë‚´ì–´ ìŠ¤ë¬´ë”©
window_history = []
suppression_counter = 0                       # ê°ì§€ í›„ ì¼ì • ì‹œê°„ ë™ì•ˆ ì¤‘ë³µ ê°ì§€ ë¬´ì‹œ
SUPPRESSION_PULL_DOWN = 6                     # ê°ì§€ í›„ 6í‹±(1.5ì´ˆ) ë™ì•ˆì€ ë¬´ì‹œ

# 1ì´ˆ ë¶„ëŸ‰(16000 íŒ¨ë”©)ì˜ ë²„í¼ í ìƒì„±
audio_buffer = np.zeros(CLIP_DURATION_SAMPLES, dtype=np.float32)

print(f"\n==== ğŸ§ [ë¼ì¦ˆë² ë¦¬íŒŒì´ìš©] ê°€ìƒ ë§ˆì´í¬(WAV ìŠ¤íŠ¸ë¦¬ë°) ì‹œì‘ ====")
print(f"ì†ŒìŠ¤ íŒŒì¼: {VIRTUAL_MIC_WAV}")
print("=========================================================\n")

# --- 4. ê°€ìƒ ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ---
try:
    with wave.open(VIRTUAL_MIC_WAV, 'rb') as wf:
        if wf.getframerate() != SAMPLE_RATE:
            print(f"ê²½ê³ : ìƒ˜í”Œ ë ˆì´íŠ¸ê°€ {SAMPLE_RATE}Hzê°€ ì•„ë‹™ë‹ˆë‹¤! ({wf.getframerate()}Hz)")
        
        total_frames = wf.getnframes()
        processed_frames = 0
        
        while processed_frames < total_frames:
            # ì‹œê°„ ì§€ì—°ìœ¼ë¡œ ì‹¤ì‹œê°„(Real-time) ë§ˆì´í¬ ìˆ˜ìŒ í™˜ê²½ ëª¨ë°©
            time.sleep(CHUNK_DURATION_MS / 1000.0)
            
            # ë§ˆì´í¬ë¡œë¶€í„° ì²­í¬(0.25ì´ˆ ë¶„ëŸ‰) ì½ì–´ì˜¤ê¸° (WAVì—ì„œ ì¶”ì¶œ)
            data = wf.readframes(CHUNK_SIZE)
            if not data:
                break
            
            # ì½ì–´ì˜¨ ë°ì´í„° ì •ê·œí™” (-1.0 ~ 1.0)
            audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
            if len(audio_chunk) < CHUNK_SIZE:
                padded = np.zeros(CHUNK_SIZE, dtype=np.float32)
                padded[:len(audio_chunk)] = audio_chunk
                audio_chunk = padded
                
            volume = np.max(np.abs(audio_chunk))
            
            # ë²„í¼ ê°±ì‹  (Sliding Window)
            audio_buffer = np.roll(audio_buffer, -CHUNK_SIZE)
            audio_buffer[-CHUNK_SIZE:] = audio_chunk
            
            # --- (A) ìˆœìˆ˜ Pythonìœ¼ë¡œ MFCC ë³€í™˜ ---
            # TensorFlow 1.15ì˜ contrib_audio.mfccì™€ 100% ë™ì¼í•˜ì§€ëŠ” ì•Šì§€ë§Œ ê·¼ì‚¬ì¹˜ë¡œ ë™ì‘í•˜ë„ë¡ ì„¸íŒ…
            # ì‹¤ì „ ë°°í¬ ì‹œì—ëŠ” ARM CMSIS-NN C++ ì½”ë“œë¡œ ëŒ€ì²´ë¨
            mfcc_feat = mfcc(audio_buffer, 
                             samplerate=SAMPLE_RATE, 
                             winlen=WINDOW_SIZE_MS/1000, 
                             winstep=WINDOW_STRIDE_MS/1000, 
                             numcep=DCT_COEFFICIENT_COUNT, 
                             nfilt=40, 
                             nfft=1024)
            # TFLite ì…ë ¥ ì°¨ì›ì— ë§ê²Œ í‰íƒ„í™” í›„ íƒ€ì… ìºìŠ¤íŒ… [1, 490]
            fingerprint = np.reshape(mfcc_feat, (1, -1)).astype(np.float32)
            
            # --- (B) MFCC -> TFLite ì¶”ë¡  ---
            interpreter.set_tensor(input_details[0]['index'], fingerprint)
            interpreter.invoke()
            output_data = interpreter.get_tensor(output_details[0]['index'])[0]
            
            # ê¸°ë¡ ì €ì¥ ë° ìŠ¤ë¬´ë”©
            window_history.append(output_data)
            if len(window_history) > 4:
                window_history.pop(0)
            
            # --- (C) ì˜ˆì¸¡ ìŠ¤ë¬´ë”© ë° í¬ì°© ë¡œì§ ---
            if len(window_history) == 4:
                smoothed_output = np.mean(window_history, axis=0)
                top_index = np.argmax(smoothed_output)
                top_score = smoothed_output[top_index]
                prediction = LABELS[top_index]
                
                # í˜„ì¬ ìŠ¤íŠ¸ë¦¬ë° ì‹œê°„ ê³„ì‚°
                current_time_sec = (processed_frames + CHUNK_SIZE) / SAMPLE_RATE
                time_str = f"[{int(current_time_sec//60):02d}:{int(current_time_sec%60):02d}]"
                
                if suppression_counter > 0:
                    suppression_counter -= 1
                    print(f"{time_str} (ê°ì§€ ëŒ€ê¸° ì¤‘... ë³¼ë¥¨: {volume:.2f})           ", end='\r', flush=True)
                else:
                    if volume < 0.05:
                        prediction = '_silence_'
                    
                    if top_score > 0.8 and prediction in ['yes', 'no']:
                        print(f"\nğŸ”¥ {time_str} í¬ì°©ë¨: '{prediction}' (ì‹ ë¢°ë„: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})")
                        # (TODO) ì´ ë¸”ë¡ì— TCP ì†Œì¼“ [TRIGGER_ON ì „ì†¡] ì¶”ê°€ êµ¬í˜„ (rasp_socket_practice.py ì½”ë“œ ë³‘í•©)
                        suppression_counter = SUPPRESSION_PULL_DOWN
                    else:
                        print(f"{time_str} ({prediction}: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})           ", end='\r', flush=True)

            processed_frames += CHUNK_SIZE

    print("\n\n==== ğŸ¤ ê°€ìƒ ìŠ¤íŠ¸ë¦¬ë° íŒŒì¼ ì¬ìƒ ì¢…ë£Œ ====")
    
except FileNotFoundError:
    print(f"\n[ì˜¤ë¥˜] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VIRTUAL_MIC_WAV}")
except KeyboardInterrupt:
    print("\n\n==== ğŸ›‘ ê°€ìƒ ë§ˆì´í¬ KWS ê°•ì œ ì¢…ë£Œ ====")
