import pyaudio
import numpy as np
import tensorflow as tf
from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
import collections
import os
import sys

# --- 1. íŒŒë¼ë¯¸í„° ì„¤ì • ---
SAMPLE_RATE = 16000
CHUNK_DURATION_MS = 250
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)
WINDOW_SIZE_MS = 40.0
WINDOW_STRIDE_MS = 20.0
DCT_COEFFICIENT_COUNT = 10
CLIP_DURATION_SAMPLES = SAMPLE_RATE

LABELS = ['_silence_', '_unknown_', 'quiz', 'understand']
CHECKPOINT_DIR = "./work/ds_cnn_korean/best"

# --- 2. TF ì„¸ì…˜ ë° ê·¸ë˜í”„ êµ¬ì¶• ---
tf.reset_default_graph()
sess = tf.InteractiveSession()

# ì…ë ¥ í”Œë ˆì´ìŠ¤í™€ë”
wav_placeholder = tf.placeholder(tf.float32, [CLIP_DURATION_SAMPLES, 1])

# MFCC ì „ì²˜ë¦¬ ê·¸ë˜í”„
spectrogram = contrib_audio.audio_spectrogram(
    wav_placeholder,
    window_size=int(SAMPLE_RATE * WINDOW_SIZE_MS / 1000),
    stride=int(SAMPLE_RATE * WINDOW_STRIDE_MS / 1000),
    magnitude_squared=True)
mfcc_op = contrib_audio.mfcc(
    spectrogram,
    SAMPLE_RATE,
    dct_coefficient_count=DCT_COEFFICIENT_COUNT)
mfcc_flatten = tf.reshape(mfcc_op, [1, -1]) # [1, 490]

# --- 3. DS-CNN ì•„í‚¤í…ì²˜ ì¬êµ¬ì¶• (ì²´í¬í¬ì¸íŠ¸ íŒŒë¼ë¯¸í„° ë¡œë”©ìš©) ---
# NOTE: To load a .ckpt without freezing, we must recreate the exact same network graph.
# Since importing models.py might be tricky dynamically, we can use the original project's models module.
try:
    import models
except ImportError:
    print("models.py not found in current directory. Please run this script from KWS-DS-CNN-for-embedded root.")
    sys.exit(1)

model_settings = models.prepare_model_settings(
      len(LABELS), SAMPLE_RATE, 1000, WINDOW_SIZE_MS,
      WINDOW_STRIDE_MS, DCT_COEFFICIENT_COUNT, 32)

fingerprint_input = tf.placeholder(
      tf.float32, [None, model_settings['fingerprint_size']], name='fingerprint_input')

# '5 64 10 4 2 2 64 3 3 1 1 64 3 3 1 1 64 3 3 1 1 64 3 3 1 1'
model_size_info = [5, 64, 10, 4, 2, 2, 64, 3, 3, 1, 1, 64, 3, 3, 1, 1, 64, 3, 3, 1, 1, 64, 3, 3, 1, 1]

logits = models.create_model(
    fingerprint_input=fingerprint_input,
    model_settings=model_settings,
    model_architecture="ds_cnn",
    model_size_info=model_size_info,
    is_training=False)

# Softmax for probabilities
probabilities_op = tf.nn.softmax(logits, name='labels_softmax')

# Load the weights from the latest best checkpoint
saver = tf.train.Saver(tf.global_variables())
checkpoint_state = tf.train.get_checkpoint_state(CHECKPOINT_DIR)
if not checkpoint_state or not checkpoint_state.model_checkpoint_path:
    print("No checkpoint found in", CHECKPOINT_DIR)
    sys.exit(1)
print(f"Loading checkpoint: {checkpoint_state.model_checkpoint_path}")
saver.restore(sess, checkpoint_state.model_checkpoint_path)

# --- 4. ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¬ë° ì„¸íŒ… ---
window_history = collections.deque(maxlen=2)
suppression_counter = 0
SUPPRESSION_PULL_DOWN = 6

audio_buffer = np.zeros(CLIP_DURATION_SAMPLES, dtype=np.float32)

p = pyaudio.PyAudio()
stream = p.open(format=pyaudio.paInt16,
                channels=1,
                rate=SAMPLE_RATE,
                input=True,
                frames_per_buffer=CHUNK_SIZE)

print("\n==== ğŸ¤ ì‹¤ì‹œê°„ ë§ˆì´í¬ KWS(í•œêµ­ì–´ .ckpt ì¶”ë¡ ) ì‹œì‘ ====")
print("('í€´ì¦ˆ' ë˜ëŠ” 'ì´í•´í•˜ì…¨ë‚˜ìš”'ë¥¼ ë§í•´ë³´ì„¸ìš”!)")
print("=========================================================")

try:
    while True:
        data = stream.read(CHUNK_SIZE, exception_on_overflow=False)
        audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
        volume = np.max(np.abs(audio_chunk))
        
        audio_buffer = np.roll(audio_buffer, -CHUNK_SIZE)
        audio_buffer[-CHUNK_SIZE:] = audio_chunk
        
        # 1. 1ì´ˆ ì˜¤ë””ì˜¤ -> MFCC Feature ì¶”ì¶œ
        mfcc_feat = sess.run(mfcc_flatten, feed_dict={wav_placeholder: audio_buffer.reshape(-1, 1)})
        
        # 2. MFCC -> Logits -> Probabilities (using the restored DS-CNN)
        probs = sess.run(probabilities_op, feed_dict={fingerprint_input: mfcc_feat})[0]
        
        window_history.append(probs)
        
        if len(window_history) == window_history.maxlen:
            smoothed_output = np.mean(window_history, axis=0)
            top_index = np.argmax(smoothed_output)
            top_score = smoothed_output[top_index]
            prediction = LABELS[top_index]
            
            if suppression_counter > 0:
                suppression_counter -= 1
                msg = f"  (ê°ì§€ ë³´ë¥˜ ì¤‘... ë³¼ë¥¨: {volume:.2f})           "
                print(msg, end='\r', flush=True)
                continue
            
            if volume < 0.02:
                prediction = '_silence_'
            
            # íƒ€ê²Ÿ í•œêµ­ì–´ í¬ì°© í™•ì¸ (ë‹¨ì–´ë³„ ë…ë¦½ ë¯¼ê°ë„ ì ìš©)
            # 'quiz'ëŠ” ë¹„êµì  ëª…í™•í•˜ê²Œ ì¡íˆë¯€ë¡œ 0.5 ìœ ì§€, 'understand'ëŠ” ê¸¸ì–´ì„œ ì ìˆ˜ê°€ ë¶„ì‚°ë˜ë¯€ë¡œ 0.45ë¡œ ë” ë¯¼ê°í•˜ê²Œ ì„¤ì •
            if (prediction == 'quiz' and top_score > 0.4) or (prediction == 'understand' and top_score > 0.45):
                print(f"ğŸ”¥ í¬ì°©ë¨: '{prediction}' (ì‹ ë¢°ë„: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})")
                suppression_counter = SUPPRESSION_PULL_DOWN
            else:
                msg = f"  ({prediction}: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})           "
                print(msg, end='\r', flush=True)

except KeyboardInterrupt:
    print("\n==== ğŸ›‘ ë§ˆì´í¬ KWS ì¢…ë£Œ ====")
finally:
    stream.stop_stream()
    stream.close()
    p.terminate()
    sess.close()
