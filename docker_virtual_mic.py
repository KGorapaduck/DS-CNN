import wave
import numpy as np
import tensorflow as tf
from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
import collections
import time
import os

# --- 1. íŒŒë¼ë¯¸í„° ì„¤ì • ---
SAMPLE_RATE = 16000
CHUNK_DURATION_MS = 250  # 0.25ì´ˆë§ˆë‹¤ ëª¨ë¸ ì¶”ë¡  (Overlap)
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)  # 4000 samples
WINDOW_SIZE_MS = 40.0
WINDOW_STRIDE_MS = 20.0
DCT_COEFFICIENT_COUNT = 10
CLIP_DURATION_SAMPLES = SAMPLE_RATE  # ëª¨ë¸ì˜ ì…ë ¥ ì‚¬ì´ì¦ˆëŠ” ë¬´ì¡°ê±´ 1ì´ˆ (16000)

LABELS = ['_silence_', '_unknown_', 'yes', 'no']
WAV_FILE_PATH = "virtual_mic_test1.wav"  # ì‚¬ìš©ìê°€ ë¼ì¦ˆë² ë¦¬íŒŒì´ë¡œ ì˜®ê¸´ íŒŒì¼ ì´ë¦„

if not os.path.exists(WAV_FILE_PATH):
    print(f"âŒ ì˜¤ë¥˜: '{WAV_FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    print("ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ê²½ë¡œ(í˜„ì¬ í´ë”)ì— í•´ë‹¹ wav íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit(1)

# --- 2. TFLite ëª¨ë¸ ë¡œë”© ---
# ë„ì»¤ í™˜ê²½ì´ë¯€ë¡œ CPU ê¸°ë°˜ Interpreterê°€ ì™„ë²½í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
interpreter = tf.lite.Interpreter(model_path="ds_cnn.tflite")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# --- 3. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬(MFCC)ë¥¼ ìœ„í•œ TF 1.15 ì „ìš© ê·¸ë˜í”„ êµ¬ì¶• ---
tf.reset_default_graph()
sess = tf.Session()

wav_placeholder = tf.placeholder(tf.float32, [CLIP_DURATION_SAMPLES, 1])
spectrogram = contrib_audio.audio_spectrogram(
    wav_placeholder,
    window_size=int(SAMPLE_RATE * WINDOW_SIZE_MS / 1000),
    stride=int(SAMPLE_RATE * WINDOW_STRIDE_MS / 1000),
    magnitude_squared=True)
mfcc_op = contrib_audio.mfcc(
    spectrogram,
    SAMPLE_RATE,
    dct_coefficient_count=DCT_COEFFICIENT_COUNT)
# [1, 49, 10] -> [1, 490] í‰íƒ„í™”
mfcc_flatten = tf.reshape(mfcc_op, [1, -1])

# --- 4. ì•ˆì •ì„± ë¡œì§ ì„¸íŒ… (PC ë§ˆì´í¬ ìµœì í™”ì™€ ë™ì¼) ---
window_history = collections.deque(maxlen=2)
suppression_counter = 0                       
SUPPRESSION_PULL_DOWN = 6                     

# 1ì´ˆ ë¶„ëŸ‰(16000)ì˜ ë¹ˆ ë²„í¼(ë°°ê²½ìŒ)
audio_buffer = np.zeros(CLIP_DURATION_SAMPLES, dtype=np.float32)

print(f"\n==== ğŸ§ ê°€ìƒ ë§ˆì´í¬(WAV) ì½ê¸° ì‹œì‘ ====")
print(f"ì¬ìƒ íŒŒì¼: {WAV_FILE_PATH}")
print("=====================================\n")

wf = wave.open(WAV_FILE_PATH, 'rb')

# ê²½ê³¼ ì‹œê°„ì„ ì¶”ì í•  ë³€ìˆ˜ ì¶”ê°€ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
elapsed_time_ms = 0

def format_time(ms):
    """ë°€ë¦¬ì´ˆë¥¼ ë¶„:ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    ms = int(ms)
    minutes = ms // 60000
    seconds = (ms % 60000) // 1000
    milliseconds = ms % 1000
    return f"{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

try:
    while True:
        # 1. ê°€ìƒ ë§ˆì´í¬(WAV íŒŒì¼)ì—ì„œ 0.25ì´ˆ ë¶„ëŸ‰(4000ê°œ ìƒ˜í”Œ)ì”© ì˜ë¼ì„œ ì½ê¸°
        data = wf.readframes(CHUNK_SIZE)
        
        # íŒŒì¼ì´ ëë‚¬ìœ¼ë©´ ì¢…ë£Œ
        if len(data) == 0:
            print("\n==== ğŸ WAV íŒŒì¼ ì¬ìƒ ì™„ë£Œ ====")
            break
            
        # í”„ë ˆì„ ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš°(íŒŒì¼ì˜ ë§¨ ëë¶€ë¶„) ì œë¡œ íŒ¨ë”©ìœ¼ë¡œ í¬ê¸° ë§ì¶”ê¸°
        audio_chunk_int16 = np.frombuffer(data, dtype=np.int16)
        if len(audio_chunk_int16) < CHUNK_SIZE:
            padded_chunk = np.zeros(CHUNK_SIZE, dtype=np.int16)
            padded_chunk[:len(audio_chunk_int16)] = audio_chunk_int16
            audio_chunk_int16 = padded_chunk
            
        # Int16 -> Float32 [-1.0, 1.0] ì •ê·œí™” (í•µì‹¬: í…ì„œí”Œë¡œìš° MFCC í™˜ê²½)
        audio_chunk = audio_chunk_int16.astype(np.float32) / 32768.0
        
        volume = np.max(np.abs(audio_chunk))
        
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: ë²„í¼ ì•ˆì˜ ë‚´ìš©ë¬¼ì„ ì™¼ìª½ìœ¼ë¡œ 0.25ì´ˆë§Œí¼ ë°€ê³  ë¹ˆìë¦¬ì— ë°©ê¸ˆ ì½ì€ 0.25ì´ˆ ì±„ì›Œë„£ê¸°
        audio_buffer = np.roll(audio_buffer, -CHUNK_SIZE)
        audio_buffer[-CHUNK_SIZE:] = audio_chunk
        
        # --- (A) 1ì´ˆ ë¶„ëŸ‰ ì˜¤ë””ì˜¤ -> MFCC ë³€í™˜ ---
        feed_dict = {wav_placeholder: audio_buffer.reshape(-1, 1)}
        fingerprint = sess.run(mfcc_flatten, feed_dict=feed_dict)
        
        # --- (B) MFCC -> TFLite ì¶”ë¡  ---
        interpreter.set_tensor(input_details[0]['index'], fingerprint)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])[0]
        window_history.append(output_data)
        
        # --- (C) ì˜ˆì¸¡ ìŠ¤ë¬´ë”© ë° íŒì • ---
        if len(window_history) == window_history.maxlen:
            smoothed_output = np.mean(window_history, axis=0)
            top_index = np.argmax(smoothed_output)
            top_score = smoothed_output[top_index]
            prediction = LABELS[top_index]
            
            if suppression_counter > 0:
                suppression_counter -= 1
                msg = f"  (ê°ì§€ ëŒ€ê¸° ì¤‘... ë³¼ë¥¨: {volume:.2f})"
                print(msg.ljust(50), end='\r', flush=True)
            else:
                if volume < 0.05:
                    prediction = '_silence_'
                
                # íƒ€ê¹ƒ í‚¤ì›Œë“œ(yes, no)ë¥¼ í™•ì‹¤í•˜ê²Œ(80% ì´ˆê³¼) ì¡ì€ ìˆœê°„!
                if top_score > 0.8 and prediction in ['yes', 'no']:
                    timestamp = format_time(elapsed_time_ms)
                    print(f"ğŸ”¥ [{timestamp}] í¬ì°©ë¨: '{prediction}' (ì‹ ë¢°ë„: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})")
                    
                    # ğŸ’¡ ì†Œì¼“ í†µì‹ ì„ í•˜ë ¤ë©´ ì—¬ê¸°ì— ì½”ë“œë¥¼ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤! ğŸ’¡
                    # if prediction == 'yes':
                    #     client_socket.sendall("YES!".encode())
                    
                    suppression_counter = SUPPRESSION_PULL_DOWN
                else:
                    msg = f"  ({prediction}: {top_score*100:.1f}%, ë³¼ë¥¨: {volume:.2f})"
                    print(msg.ljust(50), end='\r', flush=True)
                    
        # ê°€ìƒ ë§ˆì´í¬ì²˜ëŸ¼ ë³´ì´ë„ë¡ ì˜ë„ì ì¸ ì‹œê°„ ì§€ì—° (1ë°°ì† ì¬ìƒ)
        time.sleep(0.25)
        elapsed_time_ms += CHUNK_DURATION_MS

except KeyboardInterrupt:
    print("\n==== ğŸ›‘ ê°•ì œ ì¢…ë£Œ ====")
finally:
    wf.close()
    sess.close()
