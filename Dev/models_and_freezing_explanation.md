# models.py 구조 및 모델 배포(Freezing) 원리 설명

이 문서는 `models.py` 파일의 복잡한 구조와 모델 배포 과정에서 발생하는 최적화 원리에 대해 설명합니다.

## 1. models.py 파일이 복잡한 이유

`models.py` 파일이 일반적인 딥러닝 코드에 비해 방대하고 복잡해 보이는 기술적 이유는 다음과 같습니다.

### 1.1 다양한 아키텍처의 집합소 (Multi-Architecture Library)
본 프로젝트는 단일 모델 전용이 아니라, 구글의 'Speech Commands' 프로젝트에서 제안하는 여러 KWS 모델들을 선택해서 학습할 수 있는 라이브러리 구조를 가집니다.
- **포함 모델**: `single_fc`, `conv`, `low_latency_conv`, `svdf`, `ds_cnn` 등
- 사용자가 학습 시 옵션(`--model_architecture`)만 변경하면 즉시 다른 신경망을 구축할 수 있도록 설계되었습니다.

### 1.2 TensorFlow 1.x의 저수준(Low-level) 코딩 스타일
최신 고수준 API(Keras 등)와 달리 TF 1.15에서는 개발자가 직접 가중치와 편향 행렬을 선언하고 수학적 연산을 연결해야 했습니다.
- **변수 직접 관리**: `tf.Variable`을 사용한 수동 선언
- **세밀한 튜닝**: 임베디드 장치의 특성에 맞춰 메모리 구조나 연산 순서를 미세하게 조정하기에 유리합니다.

### 1.3 양자화(Quantization) 및 전처리 레이어 포함
단순한 형태의 신경망뿐만 아니라, 하드웨어 이식을 위한 **양자화 지원 레이어(`ds_cnn_quant`)**와 오디오 프레임 규격 검증 로직 등이 함께 포함되어 있어 코드 양이 많아집니다.

---

## 2. 모델 배포 시 무게(용량) 문제

"쓰지 않는 코드들까지 모델 파일에 포함되어 무거워지지 않는가?"라는 의문에 대한 답변입니다.

### 2.1 설계도(Code)와 실제 건물(Graph)의 차이
`models.py`는 건물을 지을 수 있는 모든 설계도를 모아놓은 **백과사전**과 같습니다.
- 학습을 시작할 때 특정 모델(예: `ds_cnn`)을 선택하면, 텐서플로우는 오직 해당 설계도 페이지만 펼쳐서 메모리에 신경망을 건설합니다.
- 나머지 사용하지 않은 모델 코드들은 단순한 텍스트일 뿐이며, 실제 구동되는 신경망 구조(Graph)에 하드웨어 자원을 소모하지 않습니다.

### 2.2 프리징(Freezing) 과정의 가지치기 (Pruning)
최종 배포용 파일(`.pb`)을 만드는 과정에서 가장 중요한 단계는 **'Pruning(가지치기)'**입니다.
- **추적**: 사용자가 지정한 최종 출력 노드(`labels_softmax`)를 기준으로, 그 결과를 만드는 데 기여한 노드들만 역추적합니다.
- **삭제**: 역추적 결과와 상관없는 다른 모델의 변수나 불필요한 연산들은 모두 버려집니다.
- **결과**: 최종적으로 생성되는 `.pb` 파일에는 우리가 선택한 모델의 핵심 데이터(가중치 및 연산식)만 딱 맞게 들어갑니다.

### 2.3 체크포인트(`ckpt`)의 효율성
학습 도중 생성되는 체크포인트 파일 역시 현재 메모리에 살아있는 변수들만 저장하기 때문에, 다른 모델의 흔적은 전혀 남지 않습니다.

## 3. 요약
`models.py`가 무거운 것은 **개발 및 실험의 편의성**을 위한 것이며, 실제 배포되는 모델의 크기와 성능에는 영향을 주지 않습니다. 모델의 용량을 줄이는 것은 코드 삭제가 아니라 가중치 데이터를 압축하는 **'양자화(Quantization)'**를 통해 이루어집니다.
